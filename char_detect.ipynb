{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply YOLO style Model For OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An effectient one-shot deep detwork for OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some searching, I didn't find any dataset with image+character label+ single char level bounding boxã€‚\n",
    "\n",
    "So the most direct solution: Create a scene text detection data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from constant import *\n",
    "from constant_char import *\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texted image generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "import os\n",
    "from char_data import Make_Char,rd,rd_font\n",
    "\n",
    "# rg_n,rg_l,rg_u,rg_c,\n",
    "\n",
    "# fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %rm /data/forge/char_detect/*\n",
    "# mc = Make_Char(img_dir = \"/data/coco/val2017/\",forge_dir=\"/data/forge/char_detect/\")\n",
    "# dl = DataLoader(mc,batch_size=4,shuffle=True)\n",
    "# mc_gen=iter(dl)\n",
    "\n",
    "# from tqdm import trange\n",
    "# t=trange(len(dl))\n",
    "\n",
    "# for i in t:_ = next(mc_gen)\n",
    "\n",
    "# pd.concat(mc.df_dicts,axis=0).to_csv(\"/data/forge/char_lbl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bbox</th>\n",
       "      <th>category_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30823</th>\n",
       "      <td>1</td>\n",
       "      <td>[75, 261, 25, 46]</td>\n",
       "      <td>0</td>\n",
       "      <td>000000195165.jpg</td>\n",
       "      <td>000000195165.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>6</td>\n",
       "      <td>[234, 15, 7, 21]</td>\n",
       "      <td>31</td>\n",
       "      <td>000000047801.jpg</td>\n",
       "      <td>000000047801.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51013</th>\n",
       "      <td>16</td>\n",
       "      <td>[262, 132, 24, 38]</td>\n",
       "      <td>60</td>\n",
       "      <td>000000329080.jpg</td>\n",
       "      <td>000000329080.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21935</th>\n",
       "      <td>1</td>\n",
       "      <td>[124, 133, 16, 42]</td>\n",
       "      <td>2</td>\n",
       "      <td>000000144984.jpg</td>\n",
       "      <td>000000144984.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25562</th>\n",
       "      <td>11</td>\n",
       "      <td>[121, 195, 19, 25]</td>\n",
       "      <td>22</td>\n",
       "      <td>000000166277.jpg</td>\n",
       "      <td>000000166277.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                bbox  category_id         file_name  \\\n",
       "30823           1   [75, 261, 25, 46]            0  000000195165.jpg   \n",
       "7548            6    [234, 15, 7, 21]           31  000000047801.jpg   \n",
       "51013          16  [262, 132, 24, 38]           60  000000329080.jpg   \n",
       "21935           1  [124, 133, 16, 42]            2  000000144984.jpg   \n",
       "25562          11  [121, 195, 19, 25]           22  000000166277.jpg   \n",
       "\n",
       "               image_id  \n",
       "30823  000000195165.jpg  \n",
       "7548   000000047801.jpg  \n",
       "51013  000000329080.jpg  \n",
       "21935  000000144984.jpg  \n",
       "25562  000000166277.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_df = pd.read_csv(\"/data/forge/char_lbl.csv\")\n",
    "\n",
    "ann_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[19, 106, 17, 28]',\n",
       "  '[44, 153, 16, 26]',\n",
       "  '[56, 217, 14, 33]',\n",
       "  '[58, 254, 15, 16]',\n",
       "  '[272, 160, 21, 35]',\n",
       "  '[276, 27, 9, 22]',\n",
       "  '[202, 265, 16, 30]',\n",
       "  '[257, 162, 16, 34]',\n",
       "  '[5, 236, 7, 14]',\n",
       "  '[29, 255, 17, 51]',\n",
       "  '[222, 267, 10, 22]',\n",
       "  '[51, 191, 15, 37]',\n",
       "  '[241, 162, 12, 40]',\n",
       "  '[184, 166, 13, 22]',\n",
       "  '[279, 178, 14, 16]',\n",
       "  '[68, 79, 14, 19]',\n",
       "  '[261, 276, 47, 46]',\n",
       "  '[154, 94, 22, 27]',\n",
       "  '[241, 261, 24, 30]',\n",
       "  '[22, 209, 39, 50]'],\n",
       " [7,\n",
       "  6,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  8,\n",
       "  1,\n",
       "  30,\n",
       "  14,\n",
       "  27,\n",
       "  16,\n",
       "  41,\n",
       "  59,\n",
       "  50,\n",
       "  50,\n",
       "  3133,\n",
       "  743,\n",
       "  1260,\n",
       "  1974])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = glob(IMG+\"/*\")\n",
    "\n",
    "rdimg = np.random.choice(urls)\n",
    "def get_id(url):\n",
    "    return url.split(\"/\")[-1]\n",
    "\n",
    "def get_bb(rdimg):\n",
    "    match = ann_df[ann_df[\"image_id\"]==get_id(rdimg)][[\"bbox\",\"category_id\"]]\n",
    "    return list(match[\"bbox\"]),list(match[\"category_id\"])\n",
    "\n",
    "get_bb(rdimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.text as text\n",
    "# %matplotlib inline\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(Image.open(rdimg))\n",
    "bbs,cids = get_bb(rdimg)\n",
    "# font_label = ImageFont.FreeTypeFont(\"/data/fonts_cn/msyh.ttf\", 5)\n",
    "for i in range(len(bbs)):\n",
    "    bb=eval(bbs[i])\n",
    "    # format of the bb: x, y, width, height\n",
    "    rect = patches.Rectangle((bb[0],bb[1]),bb[2],bb[3],linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "    # format of bb \n",
    "    ax.text(bb[0],bb[1],idx2name[cids[i]],dict({\"color\":\"#ff0000\"}))\n",
    "    # print(idx2name[cids[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data rows 89548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bbox</th>\n",
       "      <th>category_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_id</th>\n",
       "      <th>cate_id_oh</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>[294, 132, 10, 18]</td>\n",
       "      <td>51</td>\n",
       "      <td>000000011615.jpg</td>\n",
       "      <td>000000011615.jpg</td>\n",
       "      <td>51</td>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[278, 181, 11, 25]</td>\n",
       "      <td>9</td>\n",
       "      <td>000000052591.jpg</td>\n",
       "      <td>000000052591.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[277, 37, 14, 40]</td>\n",
       "      <td>8</td>\n",
       "      <td>000000469246.jpg</td>\n",
       "      <td>000000469246.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>[163, 12, 25, 25]</td>\n",
       "      <td>573</td>\n",
       "      <td>000000016010.jpg</td>\n",
       "      <td>000000016010.jpg</td>\n",
       "      <td>573</td>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[285, 226, 9, 24]</td>\n",
       "      <td>1</td>\n",
       "      <td>000000250758.jpg</td>\n",
       "      <td>000000250758.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>320</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                bbox  category_id         file_name  \\\n",
       "0          13  [294, 132, 10, 18]           51  000000011615.jpg   \n",
       "1           2  [278, 181, 11, 25]            9  000000052591.jpg   \n",
       "2           3   [277, 37, 14, 40]            8  000000469246.jpg   \n",
       "3          12   [163, 12, 25, 25]          573  000000016010.jpg   \n",
       "4           3   [285, 226, 9, 24]            1  000000250758.jpg   \n",
       "\n",
       "           image_id  cate_id_oh  width  height  \n",
       "0  000000011615.jpg          51    320     320  \n",
       "1  000000052591.jpg           9    320     320  \n",
       "2  000000469246.jpg           8    320     320  \n",
       "3  000000016010.jpg         573    320     320  \n",
       "4  000000250758.jpg           1    320     320  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_data(ann_df,shuffle=True):\n",
    "    \"\"\"\n",
    "    imgdf:\n",
    "    A dataframe about images, fields: \"id\",\"file_name\",\"height\",\"width\"\n",
    "    ann_df:\n",
    "    A dataframe about annotation, fields: \"image_id\",\"category_id\",\"bbox\",\n",
    "    The field \"bbox\" is a list of 4 values: x,y,height, width of the bounding box\n",
    "    \"\"\"\n",
    "    data_df=ann_df\n",
    "    \n",
    "    data_df[\"cate_id_oh\"] = data_df[\"category_id\"].apply(lambda x:idx2id[x])\n",
    "    data_df[\"bbox\"] = data_df[\"bbox\"].apply(lambda x:eval(x))\n",
    "    if shuffle:\n",
    "        data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "    print(\"total data rows\",len(data_df))\n",
    "    data_df[\"width\"] = data_df[\"category_id\"].apply(lambda x:WIDTH)\n",
    "    data_df[\"height\"] = data_df[\"category_id\"].apply(lambda x:HEIGHT)\n",
    "    return data_df\n",
    "\n",
    "data_df = df_data(ann_df)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_array = np.array(data_df.bbox.tolist()).astype(np.float64)\n",
    "wh_array = data_df[[\"width\",\"height\"]].as_matrix().astype(np.float64)\n",
    "\n",
    "def re_calibrate(bbox_array,wh_array):\n",
    "    \"\"\"return the resized bbox array\"\"\"\n",
    "    bb_resized = (bbox_array/np.concatenate([wh_array,wh_array],axis=-1)) *SIZE\n",
    "    \n",
    "    true_bb = bb_resized/SCALE\n",
    "    # switch xy as left top conner to center point\n",
    "    true_bb[...,:2]=true_bb[...,:2]+true_bb[...,2:]/2\n",
    "    # Labels' Anchor positions on the grid map\n",
    "    grid_bbxy = np.floor(true_bb[...,:2])\n",
    "    return bb_resized,true_bb,grid_bbxy\n",
    "\n",
    "def find_best_anchors(true_bbwh):\n",
    "    iou_score = []\n",
    "    for b in range(BOX):\n",
    "        wh_anc = np.tile(ANC_ARR[b],[true_bbwh.shape[0],1])\n",
    "        true_area = true_bbwh.prod(axis=-1)\n",
    "        anc_area = wh_anc.prod(axis=-1)\n",
    "    \n",
    "        inter_area = np.min([wh_anc,true_bbwh],axis=0).prod(axis=-1)\n",
    "    \n",
    "        union_area = true_area + anc_area - inter_area\n",
    "        iou_score.append(inter_area/union_area)\n",
    "    best_anchor_idx = np.array(iou_score).T.argmax(axis=-1)\n",
    "    return best_anchor_idx\n",
    "\n",
    "bb_resized,true_bb,grid_bbxy = re_calibrate(bbox_array,wh_array)\n",
    "true_bbxy,true_bbwh = true_bb[...,:2],true_bb[...,2:]\n",
    "best_anchor_idx = find_best_anchors(true_bbwh)\n",
    "\n",
    "min_lbl = SCALE * 0.001\n",
    "\n",
    "data_df[\"true_bb_x\"],data_df[\"true_bb_y\"],data_df[\"true_bb_w\"],data_df[\"true_bb_h\"]=true_bb[:,0],true_bb[:,1],true_bb[:,2],true_bb[:,3]\n",
    "data_df[\"true_grid_x\"],data_df[\"true_grid_y\"]=grid_bbxy[:,0],grid_bbxy[:,1]\n",
    "\n",
    "# data_df[\"true_bb_x\"]=data_df[\"true_bb_x\"]-data_df[\"true_grid_x\"]\n",
    "# data_df[\"true_bb_y\"]=data_df[\"true_bb_y\"]-data_df[\"true_grid_y\"]\n",
    "\n",
    "data_df[\"best_anchor\"]=best_anchor_idx\n",
    "data_df_ = data_df[data_df[\"true_bb_w\"]>min_lbl]\n",
    "data_df_ = data_df_[data_df_[\"true_bb_h\"]>min_lbl]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse adjust funtion to get train labels\n",
    "\n",
    "* t to b\n",
    "\n",
    "$\\large b_{x}=\\sigma(t_{x})+c_{x}$\n",
    "\n",
    "$\\large b_{y}=\\sigma(t_{y})+c_{y}$\n",
    "\n",
    "$\\large b_{w}=p_{w}e^{w}$\n",
    "\n",
    "$\\large b_{h}=p_{h}e^{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b to t\n",
    "\n",
    "$\\large t_{x}=-ln(\\frac{1}{b_{x}-c_{x}}-1)$\n",
    "\n",
    "$\\large t_{y}=-ln(\\frac{1}{b_{y}-c_{y}}-1)$\n",
    "\n",
    "$\\large t_{w}=ln(\\frac{b_{w}}{p_{w}})$\n",
    "\n",
    "$\\large t_{h}=ln(\\frac{b_{h}}{p_{h}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_model import dn121_conv\n",
    "\n",
    "dn121=dn121_conv(DN121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dn_yolo(nn.Module):\n",
    "    def __init__(self,feat_extra,feat_in):\n",
    "        super(dn_yolo,self).__init__()\n",
    "        self.feat_in = feat_in\n",
    "        self.feat_extra=feat_extra\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(self.feat_in,feat_in,kernel_size=(3,3),stride=(1,1),padding=1,bias=False)\n",
    "        self.conv_2 = nn.Conv2d(self.feat_in,feat_in,kernel_size=(3,3),stride=(1,1),padding=1,bias=False)\n",
    "        self.conv_3 = nn.Conv2d(self.feat_in,VEC_LEN*BOX,kernel_size=(1,1),stride=(1,1),padding=0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.feat_in)\n",
    "        self.bn2 = nn.BatchNorm2d(self.feat_in)\n",
    "        self.bn3 = nn.BatchNorm2d(self.feat_in)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.feat_extra(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv_3(x)\n",
    "        \n",
    "        # from: bs,channel, height, width\n",
    "        # to: bs, width, height, channel\n",
    "        x = x.permute([0,3,2,1]).contiguous().view(-1,FEAT_W,FEAT_H,BOX,VEC_LEN)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((HEIGHT,WIDTH)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([.5,.5,.5],[.5,.5,.5])\n",
    "                               ])\n",
    "trans_origin = transforms.Compose([transforms.Resize((HEIGHT,WIDTH)),\n",
    "                                transforms.ToTensor(),\n",
    "                               ])\n",
    "back2PIL = transforms.Compose([transforms.ToPILImage(mode=\"RGB\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "from datetime import datetime\n",
    "import os\n",
    "from p3self.matchbox import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import Data_Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Data_Multi(data_df=data_df_,\n",
    "                       transform=transform,\n",
    "                       trans_origin=trans_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer=Trainer(train_set,batch_size=16,print_on=5)\n",
    "model = dn_yolo(dn121,1024)\n",
    "from loss_ import yolo3_loss_on_t as yolo3_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function lambdas\n",
    "loss_func = yolo3_loss(lbd_coord=1,\n",
    "                       lbd_obj=5,\n",
    "                       lbd_noobj=1,\n",
    "                       lbd_cls=1,\n",
    "                       testing=False,train_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    torch.cuda.empty_cache()\n",
    "    model.cuda()\n",
    "    loss_func.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action(*args,**kwargs):\n",
    "    \"\"\"\n",
    "    y_s: label for scoring, because the y's bb has been transformed into t\n",
    "    \"\"\"\n",
    "    x,original, t_box, conf_, cls_, mask, cls_mask, b_box = args[0]\n",
    "    iteration=kwargs[\"ite\"]\n",
    "    # x,t_box, conf_, cls_, mask, cls_mask, b_box = Variable(x), Variable(t_box), Variable(conf_), Variable(cls_), Variable(mask), Variable(cls_mask), Variable(b_box)\n",
    "    if CUDA:\n",
    "        x,t_box, conf_, cls_, mask, cls_mask, b_box = x.cuda(),t_box.cuda(), conf_.cuda(), cls_.cuda(), mask.cuda(), cls_mask.cuda(), b_box.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_ = model(x)\n",
    "    model.x=x\n",
    "    model.y_=y_\n",
    "    \n",
    "    loss,loss_x,loss_y,loss_w,loss_h,loss_obj,loss_noobj,loss_cls = loss_func(y_,t_box, conf_, cls_, mask, cls_mask, b_box)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration%30==0:\n",
    "        y_pred = loss_func.t2b(y_)[0:1,...]\n",
    "        if CUDA:\n",
    "            y_pred = y_pred.cpu()\n",
    "        img = back2PIL(original[0])\n",
    "        printimg = plot_bb(img,data_to_df_bmark(y_pred))\n",
    "    return {\"loss\":loss.item(),\n",
    "            \"loss_x\":loss_x.item(),\n",
    "            \"loss_y\":loss_y.item(),\n",
    "            \"loss_w\":loss_w.item(),\n",
    "            \"loss_h\":loss_h.item(),\n",
    "            \"loss_obj\":loss_obj.item(),\n",
    "            \"loss_noobj\":loss_noobj.item(),\n",
    "            \"loss_cls\":loss_cls.item(),}\n",
    "\n",
    "trainer.action=action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
