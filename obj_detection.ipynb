{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's look at the dataset\n",
    "\n",
    "## And try to visualize the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my tool box for pytorch\n",
    "from p3self.matchbox import *\n",
    "from constant import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {IMG}|wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.text as text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "os.system(\"mkdir -p /data/bbsample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image table, with image url , image id, yup , that's all we need now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgdf=pd.DataFrame(jsdict[\"images\"])\n",
    "imgdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An annotation table, in this case, we use \"bbox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df=pd.DataFrame(jsdict[\"annotations\"])\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A category table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = glob(IMG+\"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the image id from image url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdimg = np.random.choice(urls)\n",
    "def get_id(url):\n",
    "    return int(url.split(\"/\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the bounding box data from annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bb(rdimg):\n",
    "    match = ann_df[ann_df[\"image_id\"]==get_id(rdimg)][[\"bbox\",\"category_id\"]]\n",
    "    return list(match[\"bbox\"]),list(match[\"category_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bb(rdimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picture boxes by loops into the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(Image.open(rdimg))\n",
    "bbs,cids = get_bb(rdimg)\n",
    "for i in range(len(bbs)):\n",
    "    bb=bbs[i]\n",
    "    # format of the bb: x, y, width, height\n",
    "    rect = patches.Rectangle((bb[0],bb[1]),bb[2],bb[3],linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "    # format of bb \n",
    "    ax.text(bb[0],bb[1],idx2name[cids[i]],dict({\"color\":\"#ff0000\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig.savefig(\"bbtest.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Only Look Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "From the paper [You Only Look Once: Unified, Real-Time Object Detection](http://arxiv.org/abs/1506.02640)\n",
    "\n",
    "We divid the image to the grid boxes of size $S*S$\n",
    "\n",
    "In each grid cell, we predict $B$ bounding boxes \n",
    "\n",
    "Each $B$ we have 5 predictions $x, y, w, h$ and confidence, \n",
    "\n",
    "$x,y$ relative to the grid box.$w, h$ relative to the entire picture.\n",
    "\n",
    "Each grid cell, we predict class probability $Pr(Class_{i}|Object)$.\n",
    "\n",
    "Then **class specified** confidence scores, when at test time we shall calculate, are:\n",
    "\n",
    "$Pr(Class_{i}|Object)*Pr(Object)* IOU^{truth}_{pred}=Pr(Class_{i})* IOU^{truth}_{pred}$\n",
    "\n",
    "IOU: **Intersection Over Union**\n",
    "\n",
    "So the prediction are encoded in a tensor of size $S*S*(B*5+C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO style with anchor box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get resized bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_structure(md):\n",
    "    \"\"\"Print out the model structure\"\"\"\n",
    "    for par in md.parameters():\n",
    "        print(par.size())\n",
    "\n",
    "def p_count(md):\n",
    "    \"\"\"count the parameters in side a pytorch module\"\"\"\n",
    "    allp=0\n",
    "    for p in md.parameters():allp+=np.product(p.data.numpy().shape)\n",
    "    return allp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_y=torch.arange(0,FEAT_W)\n",
    "\n",
    "cell_y=cell_y.repeat(FEAT_H).view(1,FEAT_H,FEAT_W,1,1)\n",
    "\n",
    "cell_x=cell_y.transpose(1,2)\n",
    "#cell_x,cell_y\n",
    "cell_grid=torch.cat([cell_x,cell_y],dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare bb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_data(imgdf,ann_df,shuffle=True):\n",
    "    \"\"\"\n",
    "    imgdf:\n",
    "    A dataframe about images, fields: \"id\",\"file_name\",\"height\",\"width\"\n",
    "    ann_df:\n",
    "    A dataframe about annotation, fields: \"image_id\",\"category_id\",\"bbox\",\n",
    "    The field \"bbox\" is a list of 4 values: x,y,height, width of the bounding box\n",
    "    \"\"\"\n",
    "    data_df=pd.merge(ann_df[[\"bbox\",\"category_id\",\"image_id\"]],\n",
    "                     imgdf[[\"id\",\"file_name\",\"height\",\"width\"]],\n",
    "                     left_on=\"image_id\",right_on=\"id\")\n",
    "    \n",
    "    data_df[\"cate_id_oh\"] = data_df[\"category_id\"].apply(lambda x:idx2id[x])\n",
    "    if shuffle:\n",
    "        data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "    print(\"total data rows\",len(data_df))\n",
    "    return data_df\n",
    "\n",
    "data_df = df_data(imgdf,ann_df)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resize the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bbox_array = np.array(data_df.bbox.tolist())\n",
    "wh_array = data_df[[\"width\",\"height\"]].as_matrix()\n",
    "\n",
    "def re_calibrate(bbox_array,wh_array):\n",
    "    \"\"\"return the resized bbox array\"\"\"\n",
    "    bb_resized = (bbox_array/np.concatenate([wh_array,wh_array],axis=-1)) *SIZE\n",
    "    \n",
    "    true_bb = bb_resized/SCALE\n",
    "    # switch xy as left top conner to center point\n",
    "    true_bb[...,:2]=true_bb[...,:2]+true_bb[...,2:]/2\n",
    "    # Labels' Anchor positions on the grid map\n",
    "    grid_bbxy = np.floor(true_bb[...,:2])\n",
    "    return bb_resized,true_bb,grid_bbxy\n",
    "\n",
    "def find_best_anchors(true_bbwh):\n",
    "    iou_score = []\n",
    "    for b in range(BOX):\n",
    "        wh_anc = np.tile(ANC_ARR[b],[true_bbwh.shape[0],1])\n",
    "        true_area = true_bbwh.prod(axis=-1)\n",
    "        anc_area = wh_anc.prod(axis=-1)\n",
    "    \n",
    "        inter_area = np.min([wh_anc,true_bbwh],axis=0).prod(axis=-1)\n",
    "    \n",
    "        union_area = true_area + anc_area - inter_area\n",
    "        iou_score.append(inter_area/union_area)\n",
    "    best_anchor_idx = np.array(iou_score).T.argmax(axis=-1)\n",
    "    return best_anchor_idx\n",
    "\n",
    "bb_resized,true_bb,grid_bbxy = re_calibrate(bbox_array,wh_array)\n",
    "true_bbxy,true_bbwh = true_bb[...,:2],true_bb[...,2:]\n",
    "best_anchor_idx = find_best_anchors(true_bbwh)\n",
    "\n",
    "min_lbl = SCALE * 0.001\n",
    "\n",
    "data_df[\"true_bb_x\"],data_df[\"true_bb_y\"],data_df[\"true_bb_w\"],data_df[\"true_bb_h\"]=true_bb[:,0],true_bb[:,1],true_bb[:,2],true_bb[:,3]\n",
    "data_df[\"true_grid_x\"],data_df[\"true_grid_y\"]=grid_bbxy[:,0],grid_bbxy[:,1]\n",
    "\n",
    "# data_df[\"true_bb_x\"]=data_df[\"true_bb_x\"]-data_df[\"true_grid_x\"]\n",
    "# data_df[\"true_bb_y\"]=data_df[\"true_bb_y\"]-data_df[\"true_grid_y\"]\n",
    "\n",
    "data_df[\"best_anchor\"]=best_anchor_idx\n",
    "data_df_ = data_df[data_df[\"true_bb_w\"]>min_lbl]\n",
    "data_df_ = data_df_[data_df_[\"true_bb_h\"]>min_lbl]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Index to onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse adjust funtion to get train labels\n",
    "\n",
    "* t to b\n",
    "\n",
    "$\\large b_{x}=\\sigma(t_{x})+c_{x}$\n",
    "\n",
    "$\\large b_{y}=\\sigma(t_{y})+c_{y}$\n",
    "\n",
    "$\\large b_{w}=p_{w}e^{w}$\n",
    "\n",
    "$\\large b_{h}=p_{h}e^{h}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b to t\n",
    "\n",
    "$\\large t_{x}=-ln(\\frac{1}{b_{x}-c_{x}}-1)$\n",
    "\n",
    "$\\large t_{y}=-ln(\\frac{1}{b_{y}-c_{y}}-1)$\n",
    "\n",
    "$\\large t_{w}=ln(\\frac{b_{w}}{p_{w}})$\n",
    "\n",
    "$\\large t_{h}=ln(\\frac{b_{h}}{p_{h}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from conv_model import dn121_conv\n",
    "\n",
    "dn121=dn121_conv(DN121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bce = nn.modules.loss.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dn_yolo(nn.Module):\n",
    "    def __init__(self,feat_extra,feat_in):\n",
    "        super(dn_yolo,self).__init__()\n",
    "        self.feat_in = feat_in\n",
    "        self.feat_extra=feat_extra\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(self.feat_in,feat_in,kernel_size=(3,3),stride=(1,1),padding=1,bias=False)\n",
    "        self.conv_2 = nn.Conv2d(self.feat_in,feat_in,kernel_size=(3,3),stride=(1,1),padding=1,bias=False)\n",
    "        self.conv_3 = nn.Conv2d(self.feat_in,VEC_LEN*BOX,kernel_size=(1,1),stride=(1,1),padding=0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.feat_in)\n",
    "        self.bn2 = nn.BatchNorm2d(self.feat_in)\n",
    "        self.bn3 = nn.BatchNorm2d(self.feat_in)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.feat_extra(x)\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv_3(x)\n",
    "        \n",
    "        # from: bs,channel, height, width\n",
    "        # to: bs, width, height, channel\n",
    "        x = x.permute([0,3,2,1]).contiguous().view(-1,FEAT_W,FEAT_H,BOX,VEC_LEN)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((HEIGHT,WIDTH)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([.5,.5,.5],[.5,.5,.5])\n",
    "                               ])\n",
    "trans_origin = transforms.Compose([transforms.Resize((HEIGHT,WIDTH)),\n",
    "                                transforms.ToTensor(),\n",
    "                               ])\n",
    "back2PIL = transforms.Compose([transforms.ToPILImage(mode=\"RGB\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "from datetime import datetime\n",
    "import os\n",
    "from p3self.matchbox import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import odData,Data_Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_set = odData(urllist = list(IMG+i for i in list(data_df[\"file_name\"])),\n",
    "#                           true_adj = true_adj,\n",
    "#                           vec_loc = vec_loc[:,np.newaxis,:],\n",
    "# #                            true_vec = true_vec,\n",
    "#                           transform = transform,\n",
    "#                            trans_origin=trans_origin,\n",
    "#                   )\n",
    "\n",
    "train_set = Data_Multi(data_df=data_df_,\n",
    "                       transform=transform,\n",
    "                       trans_origin=trans_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer=Trainer(train_set,batch_size=8,print_on=5)\n",
    "model = dn_yolo(dn121,1024)\n",
    "from loss_ import yolo3_loss_on_t as yolo3_loss\n",
    "loss_func = yolo3_loss(lbd_noobj=1,testing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    torch.cuda.empty_cache()\n",
    "    model.cuda()\n",
    "    loss_func.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def action(*args,**kwargs):\n",
    "    \"\"\"\n",
    "    y_s: label for scoring, because the y's bb has been transformed into t\n",
    "    \"\"\"\n",
    "    x,original, t_box, conf_, cls_, mask, cls_mask, b_box = args[0]\n",
    "    iteration=kwargs[\"ite\"]\n",
    "    x,t_box, conf_, cls_, mask, cls_mask, b_box = Variable(x), Variable(t_box), Variable(conf_), Variable(cls_), Variable(mask), Variable(cls_mask), Variable(b_box)\n",
    "    if CUDA:\n",
    "        x,t_box, conf_, cls_, mask, cls_mask, b_box = x.cuda(),t_box.cuda(), conf_.cuda(), cls_.cuda(), mask.cuda(), cls_mask.cuda(), b_box.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_ = model(x)\n",
    "    model.x=x\n",
    "    model.y_=y_\n",
    "    \n",
    "    loss,loss_x,loss_y,loss_w,loss_h,loss_obj,loss_noobj,loss_cls = loss_func(y_,t_box, conf_, cls_, mask, cls_mask, b_box)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "#     if iteration%20==0:\n",
    "#         print(\"t_box,\",t_box[mask==1],\"pred_box\",y_[mask==1][...,:4])\n",
    "#         print(\"b_box,\",b_box[mask==1])\n",
    "#         print(cls_[cls_mask==1])\n",
    "    \n",
    "    if iteration%30==0:\n",
    "        y_pred = loss_func.t2b(y_)[0]\n",
    "        if CUDA:\n",
    "            y_pred=y_pred.cpu()\n",
    "        img=back2PIL(original[0])\n",
    "        printimg=plot_bb(img,data_to_df(y_pred))\n",
    "    return {\"loss\":loss.data[0],\n",
    "            \"loss_x\":loss_x.data[0],\n",
    "            \"loss_y\":loss_y.data[0],\n",
    "            \"loss_w\":loss_w.data[0],\n",
    "            \"loss_h\":loss_h.data[0],\n",
    "            \"loss_obj\":loss_obj.data[0],\n",
    "            \"loss_noobj\":loss_noobj.data[0],\n",
    "            \"loss_cls\":loss_cls.data[0],}\n",
    "\n",
    "trainer.action=action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"yolo_v3.0.0.1.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"yolo_v3.0.0.1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
